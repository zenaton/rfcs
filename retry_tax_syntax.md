# Retry Task Syntax

## Summary

This RFC propose to describe how automatic retry of task will work

## Problem

As a user, we want to be able to define a way to automatically retry failed task

## Inspiration

- [Laravel](https://laravel.com/docs/master/queues#dealing-with-failed-jobs)
- [Kue](https://github.com/Automattic/kue#failure-backoff)
- [Sidekiq](https://github.com/mperham/sidekiq/wiki/Error-Handling#automatic-job-retry)
- [Celery](https://celery.readthedocs.io/en/latest/userguide/tasks.html#retrying)


## Discussion
It's quite clear for me that the parameters for retrying should be in the task definition:
- deciding what to do depends on the task
- we do not need thos parameters before the first execution really

So the main question I habe is:
- should this retry be programmatic
- should this retry be handled throug parameters?

### Programmatic proposal
Basically, a method `onFailure` would allow to decide what to do:
```php
public function onFailure()
{
    $this->retry();
}
```
That may imply to add a new method `retry` method to be able to differentiate a retry from a simple `dispatch`. To be really useful, we should probably provide a context as a parameter, such as the # of retrial, and also to add a way to delay the execution, such as
```php
public function onFailure($nbRetry)
{
    $this->wait(10*$nbRetry)->retry();
}
```
### Parametric proposal
We could an optional method 
```php
public function getRetryStrategy;
```
With values
- `none` for no automatic retries (no parameters)
- `simple` for simple retries (parameters: max retry, max delai)
- `exponential` for exponential backoff (parameters: max retry, max delay, curvature)

and a
```php
public function getRetryParameters;
```
that will return an array defining the retry strategy based on imposed mathematical formula.

### Retry Delay function
We could provide an optional retry delay method, such as
```php
public function onFailureRetryDelay(n);
```
where `n` is the number of attempts up to now (0 for first execution) and the return value is
- `false` if user does not want to retry (default)
- an integer representing the time in seconds to wait before retrying

As example, we can implement a simple [Exponential Backoff](https://dzone.com/articles/understanding-retry-pattern-with-exponential-back) or a simple retry with delay and maximum try attempts.

## Proposal

- the `onFailure` method seems to generic for me and implying some unneeded additional syntax. It can also encourage deviant behavior such as add more code within this method (note that it's still possible with `onFailureRetryDelay` nevertheless but less obvious)
- the `retryStrategy` way wanted to be simpler but is more complex and rigid due to the necessity to handle parameters. Also the user can not define his own strategy.

Due to its simplicity, I propose to use a `onFailureRetryDelay(n)` method. And to provide simple example of uses.

WARNING: if an exception occurs within the `onFailureRetryDelay` method, it should be identified as so a generate an exception to the retry procedure.

## Ids management

It's *really* difficult to handle all situations in a distributed environments, that why it's important to NOT consider *a priori* that everything is successful. At the contrary we must consider that any part of the process can fall apart and be retried (automatically or manually). So we must add an `id` to any part:
- When a Job (task or decision) is scheduled and sent to Engine, a unique `schedule_id` should be generated *by our library* and is the canonical id for this request.
- When the Engine sends a job to queues for processing, a unique `attempt_id` (aka `task_id` or `decision_id`) is generated *by the engine* and added to the meta data of the message
- When the Agent sends a job to scripts, a unique `processing_id` should be generated by the Agent (to be able to associated to `started` history events in current architecture => this needs to be refined in the future).

All history events should be stored *at is* in the datastore before handling them, to be sure to understand what happened.

Note: if - for any reason (the rabbitmq guarantee being "at least once") the messages are doubled, then 
- upward: the server could receive twice the same `schedule` command with same `schedule_id` - server should handle that situation.
- downward: an attempt could be processed twice (with different `processing_id`) - there is no way to avoid that (it can be mitigated, eg. by avoiding that the same agent will process it twice itself) except by systematically call the engine for each job (not for now)
- upward: the server could receive twice the same history event (`started`, `timedout`, `completed`, `failed`) with same `processing_id` - server should handle that situation.

## Retry implementation

### Principles
The easiest way to think about retry implementation is to not make any assumption, ie. to consider that, at any times, multiple attempts (for the same scheduled task or decision) could be queued, processing or finished, and that retry can be potentially asked by user or automatically scheduled at any time.

- Manual retry:
    - is always done instantaneous (`queued` without delay)
    - is possible if and only if, no attempts are currently queued, processing or completed
    - cancels previously scheduled attempts when done

- Automatic retry (re-scheduling) are canceled (ie. not done) if there is another attempt scheduled for sooner, queued, processing or completed. 

- Completion triggers cancelling of any scheduled attempt.

### Proposed implementation
It seems that managing jobs looks a lot like a mini-engine. 

To avoid to have a synchronous call to a database at each received message, we could have a elixir thread open for each canonical job/decision. This thread is in charge of knowing the states of all live attempts (live means not yet finished) and to apply above rules. States should be save in real time into an "infrastructure" database in case of crash of this service.

Attempt'status can be
- `scheduled`
- `queued`
- `processing` (if processing, and not yet completed)
- `failed` (if failed, and no other processing)
- `completed`

(Same attempt may be processed more than once in some edge case) 

If attempt status is `processing`, an additional `processing_timeout` is set
- `false` (default)
- `true` (if at least *live* processings are timed-out according to max processing time)

A (canonical) Job (Task or Decision) is 
- `completed` as soon as an attempt is completed
- `failed` as soon an attempt is failed and no attempt are currently scheduled, queued or processing (consequently no automatic retry are ongoing)
- `processing`, as soon as an attempt is processing and the Job is not completed or failed
- `queued` as soon as an attempt is queued, and the Job is not processing, failed or completed
- `scheduled` as soon as an attempt is scheduled and the Job is not queued, processing, failed or completed

If job status is `processing`, an additional `processing_timeout` is set
- `false` (default)
- `true` (if all processing attempts are timed-out according)

From a UI perspective the Jobs (canonical tasks or decisions) - not the attempts - are retried. 

From the Engine point of view, a job (canonical task or decision):
- is failed only when the automatic retry mechanism is finished (and can fail multiple times in case of manual retry)
- timed out ??? (not sure what to do with that - maybe it's not useful for workflows)
- is completed only once (even if multiple processing were successful)

## Implementation proposal

0) (Distribute message to servers)
1) Log message to datastore
2) Job manager
3) Engine

